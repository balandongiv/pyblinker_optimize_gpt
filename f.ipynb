{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Great! Based on your structure, here‚Äôs an updated and **organized `README.md`** that splits the process into three distinct steps:\n",
    "\n",
    "---\n",
    "\n",
    "# üëÅÔ∏è **Step 1‚Äì3: Visual Validation of Blink Annotations Against Signal Data**\n",
    "\n",
    "This is the **first phase** of the blink analysis pipeline. It is divided into **three systematic steps** to visually confirm that the human-annotated blink events (created in **CVAT**) align with the physiological signals:\n",
    "\n",
    "* üü¢ EAR (Eye Aspect Ratio)\n",
    "* üîµ EEG / EOG data\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Goal of This Phase**\n",
    "\n",
    "Human experts labeled blinks in video frames using **CVAT** by marking:\n",
    "\n",
    "* Blink **start**\n",
    "* **Minimum** (eye fully closed)\n",
    "* Blink **end**\n",
    "\n",
    "These annotations are made on a **frame basis** (e.g., 30 Hz video) and must be **mapped** to high-frequency **sample-based** physiological signals (e.g., EEG at 500‚Äì1000 Hz).\n",
    "This phase ensures the labels **visually tally** with the signal data before any downstream processing.\n",
    "\n",
    "---\n",
    "\n",
    "## üìò **Overview of the 3 Steps**\n",
    "\n",
    "| Step | Description                                      |\n",
    "| ---- | ------------------------------------------------ |\n",
    "| 1Ô∏è‚É£  | Visualize overall time-series signal (EAR + EEG) |\n",
    "| 2Ô∏è‚É£  | Plot each blink with start, min, and end markers |\n",
    "| 3Ô∏è‚É£  | Save a consolidated visual report using MNE      |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ **Step 1: Plot the Full Time-Series Signal**\n",
    "\n",
    "This helps you check for:\n",
    "\n",
    "* Overall quality of the data\n",
    "* Expected variations in EAR and EEG/EOG\n",
    "* Regions of interest for blinks\n",
    "\n",
    "```python\n",
    "raw.plot(\n",
    "    picks=['avg_ear', 'E8'],\n",
    "    block=True,\n",
    "    show_scrollbars=False,\n",
    "    title='avg_ear Blink Signal'\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß© **Step 2: Inspect Blink Intervals One-by-One**\n",
    "\n",
    "Each blink is annotated by a triplet: `start`, `min`, `end`. This step plots those over the EAR signal for **detailed inspection**.\n",
    "\n",
    "```python\n",
    "for _, row in blink_df.iterrows():\n",
    "    plot_with_annotation_lines(\n",
    "        raw=raw,\n",
    "        start_frame=row['startBlinks'],\n",
    "        end_frame=row['endBlinks'],\n",
    "        mid_frame=row['blink_min'],\n",
    "        picks='avg_ear',\n",
    "        sfreq=sfreq,\n",
    "    )\n",
    "```\n",
    "\n",
    "This lets you manually verify that annotations are consistent with the physiological signal dips and recoveries.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä **Step 3: Generate HTML Report with MNE**\n",
    "\n",
    "Instead of viewing blink events one by one, this step **automatically generates a consolidated HTML report** using **MNE‚Äôs reporting tool**.\n",
    "\n",
    "```python\n",
    "generate_blink_reports(\n",
    "    raw=raw,\n",
    "    blink_df=blink_df,\n",
    "    picks='avg_ear',\n",
    "    sfreq=sfreq,\n",
    "    output_dir='blink_reports',\n",
    "    base_filename='blink_report',\n",
    "    max_events_per_report=40\n",
    ")\n",
    "```\n",
    "\n",
    "### üìÇ Output:\n",
    "\n",
    "```\n",
    "blink_reports/\n",
    "‚îî‚îÄ‚îÄ blink_report.html\n",
    "```\n",
    "\n",
    "The report provides a scrollable interface to review blink events in bulk with annotation markers.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è **Behind the Scenes: Blink Frame Mapping**\n",
    "\n",
    "The function `extract_blink_durations(...)` ensures frame-based CVAT labels are aligned with high-frequency signal data:\n",
    "\n",
    "```python\n",
    "sample_index = (frame_index - offset) * (sfreq / video_fps)\n",
    "```\n",
    "\n",
    "This mapping converts frame labels to time-series sample indices that can be plotted accurately.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Outcome**\n",
    "\n",
    "After this 3-step validation, you will:\n",
    "\n",
    "* Be confident that CVAT annotations are correctly aligned\n",
    "* Have visual proof for documentation or audits\n",
    "* Avoid training with misaligned or faulty labels\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like this exported to an actual `README.md` file or paired with example screenshots!\n"
   ],
   "id": "7ec2f29939dcd6ff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
