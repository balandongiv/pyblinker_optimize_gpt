{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Global Mean vs. Local Behavior: The np.mean(candidate_signal) is a global mean. If the signal has significant baseline drift or low-frequency components, subtracting a global mean might not effectively center local events like blinks. A blink might be a peak relative to its immediate surroundings but still below the global mean after inversion.",
   "id": "35e67546dafb9b46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Focus on \"Sophisticated Solutions\" - Beyond Simple Mean Subtraction:\n",
    "Local Baseline Correction: Instead of a global mean, use a moving average or a more robust baseline estimation (e.g., Savitzky-Golay filter on the original EAR, then subtract). This would make the \"zero\" level more adaptive to local signal characteristics.\n",
    "If we subtract a local baseline from the original EAR (ear_corrected = ear - local_baseline(ear)), then blinks would be negative dips.\n",
    "Then invert: processed_signal = -ear_corrected. Now blinks are positive peaks relative to their local baseline. This sounds promising."
   ],
   "id": "caa44498d13c1b7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here's a breakdown of the thinking process to arrive at the suggested solutions:\n",
    "\n",
    "Understand the Core Problem:\n",
    "\n",
    "The user is analyzing EAR for blinks.\n",
    "\n",
    "They've transformed the signal: candidate_signal = -EAR; candidate_signal -= np.mean(candidate_signal).\n",
    "\n",
    "The expected behavior is that the peak of a blink (after transformation) should be a positive maximum.\n",
    "\n",
    "The actual behavior is that some blinks have a negative maximum.\n",
    "\n",
    "This implies that the np.mean(candidate_signal) (which is np.mean(-EAR)) is sometimes higher than the inverted peak value of a specific blink.\n",
    "\n",
    "Why would np.mean(-EAR) be higher than an inverted blink peak?\n",
    "\n",
    "Global Mean vs. Local Mean: The np.mean is calculated over the entire signal. If the signal has a general downward trend (meaning EAR is generally increasing, which is unusual for long periods but possible in segments), or if there are very strong, deep blinks elsewhere, the global mean might be pulled down (for -EAR, pulled up). A weaker blink might then not rise above this global mean.\n",
    "\n",
    "Signal Drifts/Trends: Baseline EAR can drift up or down due to head pose, lighting changes, fatigue, etc. If EAR drifts downwards significantly, -EAR drifts upwards. The global mean of -EAR would be high. A blink occurring during a period of low EAR (high -EAR) might not have its peak become positive after global mean subtraction.\n",
    "\n",
    "Incomplete Blinks/Noisy Data: If a \"blink\" is very shallow or noisy, its peak might genuinely be low.\n",
    "\n",
    "Epoching Issues: If the epoching is done before this normalization and the epoch is very short, the np.mean within that short epoch might behave differently than the global mean. (The user's code seems to apply this to the whole candidate_signal, so this is less likely the primary cause for this specific transformation step, but good to keep in mind for epoch-based processing).\n",
    "\n",
    "Brainstorming Solutions - Initial Ideas & Refinements:\n",
    "\n",
    "Idea 1: Local Normalization. Instead of a global mean, use a local mean or a moving average.\n",
    "\n",
    "Pro: Adapts to local signal characteristics.\n",
    "\n",
    "Con: How to choose the window for the moving average? If the window is too small, it might flatten the blink itself. If too large, it's back to a global-like mean.\n",
    "\n",
    "Refinement: Use a rolling window that is larger than a typical blink duration but smaller than long-term drifts. This can be used to estimate a baseline.\n",
    "\n",
    "Idea 2: Detrending. Remove slow-moving trends from the original EAR signal before inversion and mean subtraction.\n",
    "\n",
    "Pro: Directly addresses baseline drift.\n",
    "\n",
    "Con: Need to choose a detrending method (e.g., polynomial, Savitzky-Golay filter as a smoother).\n",
    "\n",
    "Idea 3: Adaptive Thresholding / Peak Finding. Instead of relying on the sign after global mean subtraction, perhaps the peak finding itself needs to be more robust.\n",
    "\n",
    "Pro: Focuses on the shape of the blink.\n",
    "\n",
    "Con: The user's problem is specifically about the value at the peak being positive after their transformation. This is more about ensuring the transformation works as intended.\n",
    "\n",
    "Idea 4: Per-Epoch Normalization (if epochs are already defined).\n",
    "\n",
    "If blinks are already roughly identified, normalize within each candidate blink epoch.\n",
    "\n",
    "epoch_signal_transformed = -epoch_signal\n",
    "\n",
    "epoch_signal_transformed -= np.mean(epoch_signal_transformed)\n",
    "\n",
    "Pro: Ensures that within that epoch, the peak is likely to be positive relative to the mean of that epoch.\n",
    "\n",
    "Con: The user seems to be applying the transformation to the whole signal first. Also, if an \"epoch\" doesn't actually contain a full blink, this could be misleading. The global mean subtraction is an attempt to set a common reference.\n",
    "\n",
    "Idea 5: Focus on the original EAR. Blinks are dips in EAR. The transformation -EAR - mean(-EAR) aims to make them positive peaks relative to a zero baseline. If the peak of -EAR is P, and mean(-EAR) is M, then P - M is the transformed peak. If P - M < 0, it means P < M. This means the inverted peak is less than the average inverted signal.\n",
    "\n",
    "Developing \"More Sophisticated\" Solutions:\n",
    "\n",
    "Baseline Subtraction (Pre-Inversion): This seems most direct.\n",
    "\n",
    "Estimate the baseline of the original EAR signal. This baseline represents the \"eyes open\" state.\n",
    "\n",
    "Subtract this baseline from EAR: EAR_corrected = EAR - EAR_baseline. Now, \"eyes open\" is around zero, and blinks are significant negative deflections.\n",
    "\n",
    "Then invert: candidate_signal = -EAR_corrected. Now blinks are positive peaks, and \"eyes open\" is around zero.\n",
    "\n",
    "Optional: candidate_signal -= np.mean(candidate_signal) (though np.mean(candidate_signal) should be close to zero if the baseline correction was good).\n",
    "\n",
    "How to estimate baseline?\n",
    "\n",
    "Rolling median (robust to blinks themselves).\n",
    "\n",
    "Low-pass filtering (e.g., Butterworth, Savitzky-Golay with a wide window and low polynomial order).\n",
    "\n",
    "Morphological operations (opening: erosion followed by dilation can estimate the lower envelope for EAR, which acts as a baseline).\n",
    "\n",
    "Adaptive Z-Score / Dynamic Thresholding:\n",
    "\n",
    "Instead of a simple global mean, use a rolling mean and rolling standard deviation of the original EAR.\n",
    "\n",
    "A blink could be defined as a point where EAR drops significantly below the rolling mean (e.g., EAR < rolling_mean - k * rolling_std).\n",
    "\n",
    "Then, once such regions are identified, process those regions with the inversion and local centering.\n",
    "\n",
    "Refining the Current Approach (Focusing on the Inverted Signal):\n",
    "\n",
    "The current approach is S' = -S - mean(-S).\n",
    "\n",
    "If the max of S' is negative for a blink, it means max(-S_blink) < mean(-S_global).\n",
    "\n",
    "This suggests that the \"valley\" of the blink (which becomes a \"peak\" after inversion) isn't deep enough relative to the overall average of the inverted signal.\n",
    "\n",
    "Could we adjust the mean(-S_global) part? Perhaps use a percentile? If we subtract, say, the 20th percentile of -S instead of the mean, more peaks might become positive. But this is a bit arbitrary.\n",
    "\n",
    "The baseline subtraction on original EAR is more principled.\n",
    "\n",
    "Structuring the Answer:\n",
    "\n",
    "Start by explaining why the issue occurs (global mean vs. local features).\n",
    "\n",
    "Propose solutions, starting with simpler refinements and moving to more robust ones.\n",
    "\n",
    "Solution 1: Baseline Subtraction on Original EAR: This seems the most robust and directly addresses the conceptual problem. Detail how to get the baseline.\n",
    "\n",
    "Solution 2: Per-Epoch Centering (if applicable): If blink candidates are already robustly identified, centering each one individually can work. But this defers the problem of good blink identification.\n",
    "\n",
    "Solution 3: Robust Peak Detection and Windowing: Instead of relying on the global transformation to make peaks positive, find peaks in -EAR first. Then, for each peak, define a window and then locally normalize that window if desired. This changes the order of operations.\n",
    "\n",
    "Solution 4: Modifying the Zero-Crossing Logic (as a secondary fix): The user's zero-crossing logic looks for negative values. If the peak itself is negative, this logic might fail or give unexpected results. This is a consequence, not a cause. Fixing the peak positivity should help the zero-crossing.\n",
    "\n",
    "Refining the Zero-Crossing Logic (as requested, but also as a consequence):\n",
    "\n",
    "The current logic is okay but the fallbacks could be improved or made more explicit.\n",
    "\n",
    "If left_zero or right_zero are np.nan, it means the transformed signal didn't cross zero in the expected way. This is precisely because the peak might not be positive, or the \"shoulders\" of the blink don't drop below the (transformed) baseline.\n",
    "\n",
    "If the peak is guaranteed positive, then zero crossings should exist if the blink returns to baseline.\n",
    "\n",
    "The \"extend search\" fallback is reasonable. Setting to np.nan is also a valid way to flag problematic blinks.\n",
    "\n",
    "Consider what np.nan means for downstream processing. Does it invalidate the blink?\n",
    "\n",
    "Code Examples: Provide Python snippets for the key ideas.\n",
    "\n",
    "Self-Correction/Double Check:\n",
    "\n",
    "Does \"baseline subtraction on original EAR\" directly make the inverted peak positive?\n",
    "\n",
    "EAR_baseline_corrected = EAR - baseline_EAR (blinks are negative dips)\n",
    "\n",
    "inverted_signal = -EAR_baseline_corrected (blinks are positive peaks, baseline is ~0)\n",
    "\n",
    "If we then do inverted_signal - np.mean(inverted_signal), and np.mean(inverted_signal) is close to 0 (as it should be), then the positive peaks remain positive. This looks good.\n",
    "\n",
    "Is the user's problem statement clear? \"peak of each blink epoch (max value) is a positive value\". Yes.\n",
    "\n",
    "Are the solutions practical for 1.8 million points? Yes, rolling operations and filtering are efficient.\n",
    "\n",
    "This detailed thinking process, moving from understanding the problem to brainstorming, refining, and structuring, helps in generating comprehensive and effective solutions."
   ],
   "id": "ed02f4939a78ff10"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
